{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "The pseudo-code for the algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998). \n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "Please note the following about the pseudo-code: The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The reward $r = r(s, a, s')$ is the expected immediate reward on transition from state $s$ to the next state $s'$ under action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The three problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this environment from the lectures and from your previous lab exercise. The grid squares in the figure are numbered as shown. In all three problems, the following is true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are _west_, _north_, _south_, and _east_. If the direction of movement is blocked by a wall (for example, if the agent executes action south at grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold, the agent collects the gold. In order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square. \n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains the bomb, the agent activates the bomb. \n",
    "\n",
    "** Terminal states:** The game terminates when all gold is collected or when the bomb is activated. \n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to 1 to the power of -10.\n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "Set $\\gamma=1$.\n",
    "\n",
    "Reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic environment\n",
    "\n",
    "In this part, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment. \n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "The array `policy` is an array of strings that specifies an optimal action at each grid location. \n",
    "\n",
    "The array `v` is an array of floats that contains the expected return at each grid square (that is, the state value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# define a function that used to make step\n",
    "def make_step(current_state, action):\n",
    "    # when action == North, get next state\n",
    "    if action == 'n':\n",
    "        next_state = current_state + 5\n",
    "        if next_state >= 25:\n",
    "            next_state = current_state\n",
    "    \n",
    "    # when action == East, get next state\n",
    "    if action == 'e':\n",
    "        next_state = current_state + 1\n",
    "        if next_state % 5 == 0:\n",
    "            next_state = current_state\n",
    "    \n",
    "    # when action == South, get next state and rewards\n",
    "    if action == 's':\n",
    "        next_state = current_state - 5\n",
    "        if next_state < 0:\n",
    "            next_state = current_state\n",
    "\n",
    "    # when action == West, get next state and rewards\n",
    "    if action == 'w':\n",
    "        next_state = current_state - 1\n",
    "        if next_state % 5 == 4:\n",
    "            next_state = current_state\n",
    "            \n",
    "    return next_state\n",
    "\n",
    "\n",
    "# define a function that calculate the max and argmax update value\n",
    "def update(current_state, probability):\n",
    "    ret = []\n",
    "    probability = probability + (1 - probability) / len(actions)\n",
    "    \n",
    "    # loop for each action\n",
    "    for action in actions:\n",
    "        # main intend action\n",
    "        a1_next_state = make_step(current_state, action)\n",
    "        a1 = probability * (rewards[a1_next_state] + gamma * v[a1_next_state])\n",
    "        \n",
    "        sub_actions = actions[actions != action]\n",
    "        for sub_action in sub_actions:\n",
    "            # the other three random action, and get an expectation(sum)\n",
    "            a2_next_state = make_step(current_state, sub_action)\n",
    "            a2 = (1 - probability) / (len(actions)-1) * (rewards[a2_next_state] + gamma * v[a2_next_state])\n",
    "            a1 += a2\n",
    "        ret.append(a1)\n",
    "            \n",
    "    return np.array(ret).max(), np.array(ret).argmax()\n",
    "\n",
    "\n",
    "# value iteration algorithm simulation\n",
    "def value_iteration(probability):\n",
    "    # value iteration algorithm\n",
    "    delta = 1\n",
    "    while delta >= theta:\n",
    "        delta = 0    \n",
    "        for state in s:\n",
    "            current_v = v[state]\n",
    "            v[state] = update(state, probability)[0]\n",
    "            delta = np.array([delta, np.abs(current_v-v[state])]).max()\n",
    "\n",
    "    # get the policy\n",
    "    policy = []\n",
    "    for state in s_plus:\n",
    "        policy.append(actions[update(state, probability)[1]])\n",
    "    \n",
    "    return v.reshape(5,5)[::-1], np.array(policy).reshape(5,5)[::-1]\n",
    "\n",
    "\n",
    "# initial default parameters\n",
    "theta = 1e-10\n",
    "gamma = 1\n",
    "actions = np.array(['n','e','s','w'])\n",
    "s_plus = np.arange(25)\n",
    "s = np.delete(s_plus, [18,23])\n",
    "v = np.random.rand(25)\n",
    "v[[18,23]] = 0\n",
    "rewards = np.ones(25) * (-1)\n",
    "rewards[[18,23]] = [-11, 9]\n",
    "\n",
    "v, policy = value_iteration(probability = 1)\n",
    "print(policy)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic environment\n",
    "\n",
    "In this part, the agent is not always able to execute its actions as intended. With probability 0.8, it moves in the intended direction. With probability 0.2, it moves in a random direction. For example, from grid square 0, if the agent executes action _north_, with probability 0.8, the action will work as intended. But with probability 0.2, the agent's motor control system will move in a random direction (including north). For example, with probability 0.05, it will try to move west (where it will be blocked by the wall and hence remain in grid square 0). Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "[[6.04169329 7.28756636 8.61359951 0.         8.69262311]\n",
      " [4.86185111 5.99087587 6.37082431 0.         6.46721593]\n",
      " [3.67550938 4.69621388 4.99441863 3.2189158  5.10250988]\n",
      " [2.48699534 3.40945989 3.66922967 2.64122933 3.78610115]\n",
      " [1.35979208 2.19733672 2.42878751 1.57272161 2.55202451]]\n"
     ]
    }
   ],
   "source": [
    "# initial the v \n",
    "v = np.random.rand(25)\n",
    "v[[18,23]] = 0\n",
    "\n",
    "v, policy = value_iteration(probability = 0.8)\n",
    "print(policy)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic environment with two pieces of gold\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this part, the environment is identical to the environment in the first part with the following exception: there is an additional piece of gold on grid square 12. Recall from earlier instructions that the terminal state is reached only when **_any_** gold is collected or when the bomb is activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['e' 'e' 's' 'n' 'n']\n",
      " ['e' 'e' 'n' 'w' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [7. 8. 9. 0. 8.]\n",
      " [8. 9. 0. 9. 8.]\n",
      " [7. 8. 9. 8. 7.]\n",
      " [6. 7. 8. 7. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# initial default parameters\n",
    "s_plus = np.arange(25)\n",
    "s = np.delete(s_plus, [12,18,23])\n",
    "v = np.random.rand(25)\n",
    "v[[12,18,23]] = 0\n",
    "rewards = np.ones(25) * (-1)\n",
    "rewards[[12,18,23]] = [9,-11,9]\n",
    "\n",
    "v, policy = value_iteration(probability = 1)\n",
    "print(policy)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic environment with two pieces of gold\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this part, the environment is identical to the environment in the first part with the following exception: there is an additional piece of gold on grid square 12. Recall from earlier instructions that the terminal state is reached only when **_all_** gold is collected or when the bomb is activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait to be implemented\n",
    "\n",
    "In this part, unlike the any gold is collected is a terminal state before, it needs to collect all the gold to terminal the game, so need to change the state representation into all situations.\n",
    "The results is shown in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e' 'e' 'e' 'w' 'w']\n",
      " ['e' 'e' 's' 'n' 'n']\n",
      " ['e' 'e' 'n' 'w' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "[[14. 15. 16.  7. 16.]\n",
      " [14. 15. 16.  0. 15.]\n",
      " [15. 16.  7. 16. 15.]\n",
      " [14. 15. 16. 15. 14.]\n",
      " [13. 14. 15. 14. 13.]]\n"
     ]
    }
   ],
   "source": [
    "solution_policy = np.array([['e','e','e','w','w'],['e','e','s','n','n'],['e','e','n','w','w'],\n",
    "                            ['n','n','n','n','n'],['n','n','n','n','n']])\n",
    "\n",
    "print(solution_policy)\n",
    "\n",
    "solution_v = np.array([[14,15,16,7,16],[14,15,16,0,15],[15,16,7,16,15],\n",
    "                       [14,15,16,15,14],[13,14,15,14,13]]).astype(float)\n",
    "\n",
    "print(solution_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
